# FFB Database - Projet Bridge

Base de donn√©es pour la F√©d√©ration Fran√ßaise de Bridge (FFB) utilisant Prisma et Supabase.

## üöÄ Configuration

### 1. Installation des d√©pendances
```bash
npm install
```

### 2. Configuration de l'environnement
Copiez le fichier `.env.example` vers `.env` et configurez vos variables d'environnement :

```env
# === SUPABASE ===
SUPABASE_URL=https://votre-projet.supabase.co
SUPABASE_ANON_KEY=votre-cl√©-anon
SUPABASE_SERVICE_ROLE_KEY=votre-cl√©-service

# === POSTGRESQL ===
DATABASE_URL="postgresql://postgres:votre-mot-de-passe@votre-host:5432/postgres"
```

### 3. G√©n√©ration du client Prisma
```bash
npm run db:generate
```

## üìä Mod√®les de donn√©es

### Club
- Informations de base (nom, code, adresse, etc.)
- Relations avec joueurs, r√¥les et agr√©ments

### Joueur
- Informations personnelles (nom, pr√©nom, licence, etc.)
- Relation avec le club d'appartenance
- Relations avec r√¥les et agr√©ments

### Role
- D√©finition des r√¥les/fonctions
- Relations avec joueurs et clubs

### Agrement
- D√©finition des agr√©ments/autorisations
- Relations avec joueurs et clubs

## üîß Scripts disponibles

### Base de donn√©es
```bash
# G√©n√©rer le client Prisma
npm run db:generate

# R√©cup√©rer le sch√©ma de la base
npm run db:pull

# Cr√©er une migration
npm run db:migrate

# Ouvrir Prisma Studio
npm run db:studio
```

### Import/Export
```bash
# Importer les donn√©es CSV
npm run import:data

# Tester la connexion
npm run test:connection
```

## üìÅ Structure du projet

```
FFB_Database/
‚îú‚îÄ‚îÄ prisma/
‚îÇ   ‚îî‚îÄ‚îÄ schema.prisma          # Sch√©ma de la base de donn√©es
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ import-data.js         # Script d'import CSV
‚îÇ   ‚îî‚îÄ‚îÄ test-connection.js     # Script de test
‚îú‚îÄ‚îÄ clubs.csv                  # Donn√©es des clubs
‚îú‚îÄ‚îÄ players.csv                # Donn√©es des joueurs
‚îî‚îÄ‚îÄ .env                       # Variables d'environnement
```

## üö® R√©solution des probl√®mes

### Probl√®me de connexion
Si vous obtenez une erreur de connexion :
1. V√©rifiez que votre projet Supabase est actif
2. V√©rifiez les variables d'environnement dans `.env`
3. Testez la connexion avec `npm run test:connection`

### Probl√®me d'import
Si l'import √©choue :
1. V√©rifiez que les fichiers CSV sont dans le bon format
2. V√©rifiez que la base de donn√©es est accessible
3. Consultez les logs d'erreur

## üìù Notes

- Les fichiers `.env`, `.gitignore` et `.cursorignore` sont ignor√©s par Git
- Le client Prisma est g√©n√©r√© automatiquement apr√®s modification du sch√©ma
- Les scripts d'import g√®rent automatiquement les relations entre les tables

## üìö FFB-Specific Data Model & Schema Reference

For historical and reference purposes, the following files document the original FFB data model and SQL schema used for scraping and legacy import:

- [docs/FFB_data_schema.md](docs/FFB_data_schema.md): Detailed documentation of the FFB data model (clubs, players, roles, etc.)
- [scripts/ffb_schema.sql](scripts/ffb_schema.sql): Original FFB SQL schema (not used in production)

> **Note:** The main schema for your Supabase/PostgreSQL database is defined in [schema.sql](schema.sql) at the root. Always keep this file up to date with your live database structure.

## üö¶ Scraper Progress & Status (July 2025)

- Scrapes all Lorraine clubs/entities (skipping Comit√© de Lorraine and first 3 dropdown entries)
- For each club: visits info page, robustly saves clubs.csv after each entity
- For each club: scrapes all members (current and renewed), handles 'Sympathisant' (licence ending with 's')
- Outputs tab-delimited CSVs for both clubs and players
- Saves players.csv after each club to minimize data loss
- Known issue: error when navigating to member section (under investigation)
- Script is robust for large-scale scraping, but under active development for further detail and error handling

## üö¶ Scraper Refactor Plan (July 2025)

- **Batch-first scraping:**
  - Scrape all clubs/entities from the dropdown, collecting all info from the /informations page and FFB entity ID.
  - Build the clubs DataFrame with all fields encountered, dynamically adding columns as new fields are found (missing values filled with '').
  - Save the full clubs.csv only after all entities are processed.
  - Print progress (e.g., "Comit√© de Lorraine - Collecte des Clubs - Trait√©s n/total").
- **Member scraping:**
  - For each club, build the URL for the members list and scrape all members, always including the FFB licensee ID.
  - Build the players DataFrame, dynamically adding columns as new fields are found (missing values filled with '').
  - Save the full players.csv only after all members are processed.
- **Section-by-section deep scraping:**
  - For each section/tab, iterate through the relevant DataFrame, build the URL, and scrape all new fields, updating the DataFrame and saving after each section.
  - Print progress for each section and entity/member.
- **Dynamic DataFrame handling:**
  - As new fields are scraped, add them to the DataFrame if they don‚Äôt exist, filling missing values for previous rows with ''.
  - Final CSVs always have all columns seen so far, matching the evolving DB schema.
- **Navigation optimization:**
  - Batch scraping and direct URL building minimize navigation time.
- **Scalability:**
  - Approach is robust for all committees and the full FFB dataset (1050 clubs, 100,000+ members).
- **Error handling & data safety:**
  - Save CSVs after each major batch/section to avoid data loss.
  - Print errors and continue processing to maximize data collection.

> The script is being refactored to follow this plan for maximum efficiency, robustness, and scalability.

## üõ°Ô∏è Robust Scraper Checkpointing & Recovery (2025)

- The scraper saves the CSV after each major section (e.g., "Informations principales") is scraped for all entities.
- On error for an entity, the error and entity ID are logged, the entity is skipped, and scraping continues.
- On crash or interruption, the scraper saves progress before exiting.
- On restart, the scraper resumes from the last successfully scraped entity (using the URL ID or a progress log).
- Timing and debug prints are provided for each major block per entity.
- The CSV always contains the full entity IDs as scraped (with all digits/leading zeros) and all relevant fields.
- An error log file is maintained for skipped entities or sections.
- This hybrid checkpointing approach ensures minimal data loss, easy recovery, and robust, maintainable scraping.

# üìù Project Progress & Robust Scraping Architecture (2025)

- **Dual-URL member scraping and status merging**: The scraper now collects member data from both the full list (encaissement) and the renewed/consultation list, merging statuses for completeness.
- **CSV-based parsing**: Member tables are parsed as tab-delimited CSV for reliability and speed, with robust header detection and status inference.
- **Robust error handling**: The scraper gracefully handles missing OPGButtons, absent or empty tables, and navigation errors, logging issues and skipping problematic entities without hanging.
- **Modular scraping functions**: Each entity type (FFB, Zone, etc.) will have its own modular scraping function, with a clear, commented skeleton for each section (informations principales, acteurs, tableau de bord, etc.).
- **Clear separation of logic**: The codebase is structured so that each section/tab of the FFB site is scraped by a dedicated function, making it easy to adapt to new entity types or changes in the site structure.
- **Checkpointing and recovery**: The scraper saves progress after each major section, can resume from the last successful entity, and maintains error logs for skipped entities or sections.
- **API key hygiene**: Guidance is provided for handling public API key leaks, including revoking, deleting, and purging secrets from git history, and ensuring sensitive files are not tracked.

---
